import hashlib
import json
import os
from datetime import datetime
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed

from dotenv import load_dotenv

from langchain_community.document_loaders import (
    Docx2txtLoader,
    PyPDFLoader,
    UnstructuredPowerPointLoader,
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

# =========================
# Configuration
# =========================

# Base directory: folder where this script lives (put your docs under here)
BASE = os.path.dirname(os.path.abspath(__file__))

# Manifest and chroma directory live next to this script
MANIFEST = os.path.join(BASE, ".docx_manifest.json")
PERSIST_DIR = os.path.join(BASE, "chroma_chat")
print(f"Base directory: {BASE}")

COLLECTION = "demo"
EMBED_MODEL = "text-embedding-3-small"
CHUNK_SIZE = 1200
CHUNK_OVERLAP = 200
MAX_WORKERS = 8

# Supported extensions
EXTS = {".docx", ".pdf", ".pptx"}

# Load .env from BASE so OpenAIEmbeddings can find OPENAI_API_KEY
load_dotenv(os.path.join(BASE, ".env"))


# =========================
# Utilities
# =========================

def norm_hash(text: str) -> str:
    """Hash of normalized whitespace."""
    return hashlib.sha256(" ".join(text.split()).encode("utf-8")).hexdigest()


def normalize_text(s: str) -> str:
    return " ".join(s.split()).strip()


def chunk_id(chunk_text: str) -> str:
    return hashlib.sha256(normalize_text(chunk_text).encode("utf-8")).hexdigest()


def rel_key(p: str) -> str:
    """Uniform relative key for manifest and bookkeeping."""
    return os.path.relpath(p, BASE).replace("\\", "/")


def file_type(rel_or_path: str) -> str:
    ext = os.path.splitext(rel_or_path)[1].lower()
    if ext == ".docx":
        return "docx"
    if ext == ".pdf":
        return "pdf"
    if ext == ".pptx":
        return "pptx"
    return "other"


def load_manifest(path: str) -> dict:
    if not os.path.exists(path):
        return {"files": {}}
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def save_manifest(path: str, data: dict) -> None:
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    os.replace(tmp, path)


# ----- Loaders for hashing (whole-file text) -----

def read_text_docx(abs_path: str) -> str:
    return Docx2txtLoader(abs_path).load()[0].page_content


def read_text_pdf(abs_path: str) -> str:
    # Single-document extraction
    doc = PyPDFLoader(abs_path, mode="single").load()[0]
    return doc.page_content


def read_text_pptx(abs_path: str) -> list[str]:
    # For PPTX, we’ll return a list of slide texts (one per slide).
    slides = UnstructuredPowerPointLoader(abs_path).load()
    # Some loaders may include empty slides; keep them for stability but they will hash distinctly.
    return [d.page_content or "" for d in slides]


def read_text_for_hash(abs_path: str, ftype: str) -> str:
    if ftype == "docx":
        return read_text_docx(abs_path)
    if ftype == "pdf":
        return read_text_pdf(abs_path)
    if ftype == "pptx":
        # For hashing/rename detection, hash the whole deck by concatenating slide texts.
        slide_texts = read_text_pptx(abs_path)
        return "\n\n".join(slide_texts)
    raise ValueError(f"Unsupported file type for hashing: {ftype}")


# ----- Chunking for embeddings -----

def chunk_text_generic(text: str):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        add_start_index=True,
    )
    docs = splitter.split_documents([Document(page_content=text)])
    return [(d.page_content, d.metadata.get("start_index", None)) for d in docs]


def build_docs_for_file(rel_path: str, file_hash: str, mtime_ns: int, size: int):
    """
    Build (ids, docs) for a given file depending on its type.
    - DOCX: load once, split with RecursiveCharacterTextSplitter
    - PDF: load all pages, concatenate, split with RecursiveCharacterTextSplitter
    - PPTX: one chunk per slide (split per page/slide as requested)
    """
    full_path = os.path.abspath(os.path.join(BASE, rel_path))
    ftype = file_type(rel_path)

    ids, docs = [], []

    if ftype == "docx":
        text = read_text_docx(full_path)
        chunks = chunk_text_generic(text)

    elif ftype == "pdf":
        text = read_text_pdf(full_path)  # one whole blob
        chunks = chunk_text_generic(text)

    elif ftype == "pptx":
        slide_texts = read_text_pptx(full_path)  # list[str]
        # One chunk per slide. Use slide index as start_index and also store page_number.
        chunks = []
        for i, stext in enumerate(slide_texts):
            chunks.append((stext, i))  # start_index = slide index

    else:
        raise ValueError(f"Unsupported file type: {rel_path}")

    for ctext, start in chunks:
        cid = chunk_id(ctext)
        ids.append(cid)
        meta = {
            "source_path": full_path.replace("\\", "/"),
            "filename": os.path.basename(rel_path),
            "file_text_hash": file_hash,
            "chunk_text_hash": cid,   # same as id
            "start_index": start,
            "mtime": mtime_ns,
            "size": size,
            "ingest_version": f"rcs:{CHUNK_SIZE}:{CHUNK_OVERLAP}|{EMBED_MODEL}",
            "type": ftype,
        }
        # For PPTX, also include page_number mirroring start_index (slide index)
        if ftype == "pptx":
            meta["page_number"] = (start + 1) if isinstance(start, int) else None  # 1-based
        docs.append(Document(page_content=ctext, metadata=meta))

    return ids, docs


# =========================
# PART 1: Detection
# =========================

def detect_changes():
    prev_manifest = load_manifest(MANIFEST)
    prev = prev_manifest.get("files", {})

    # 1) Scan supported files (fast pass, NO subfolders)
    curr_stats = {}
    for name in os.listdir(BASE):
        ext = os.path.splitext(name)[1].lower()
        if ext not in EXTS:
            continue
        if name.startswith("~$"):   # skip temp/lock files
            continue
        p_abs = os.path.join(BASE, name)
        rel = rel_key(p_abs)
        try:
            s = os.stat(p_abs)
        except FileNotFoundError:
            continue
        curr_stats[rel] = {"mtime": s.st_mtime_ns, "size": s.st_size}

    # 2) Decide which files need parsing (new or changed by stat)
    to_parse = []
    for rel, st in curr_stats.items():
        old = prev.get(rel)
        if old is None or old.get("mtime") != st["mtime"] or old.get("size") != st["size"]:
            to_parse.append(rel)

    # 3) Parse only those; compute content hash depending on file type
    def parse_and_hash(rel):
        try:
            abs_path = os.path.join(BASE, rel)
            ftype = file_type(rel)
            text_for_hash = read_text_for_hash(abs_path, ftype)
            return rel, norm_hash(text_for_hash)
        except Exception as e:
            print(f"[WARN] Failed to parse: {rel} ({e})")
            return rel, None

    new_hashes = {}
    if to_parse:
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
            futures = [ex.submit(parse_and_hash, rel) for rel in to_parse]
            for fut in as_completed(futures):
                rel, h = fut.result()
                if h is not None:
                    new_hashes[rel] = h

    # 4) Classify changes
    added, modified, same_count = [], [], 0
    for rel in to_parse:
        if rel not in new_hashes:
            # failed parse: skip classification; keep previous state if exists
            continue
        old = prev.get(rel)
        if old is None:
            added.append(rel)
        elif new_hashes[rel] != old.get("text_hash"):
            modified.append(rel)
        else:
            same_count += 1

    deleted = sorted(set(prev) - set(curr_stats))
    unchanged = (len(curr_stats) - len(to_parse)) + same_count

    # 4b) Renames/moves by matching content hashes (added ↔ deleted)
    deleted_by_hash = defaultdict(list)
    for rel in deleted:
        h = prev[rel].get("text_hash")
        if h:
            deleted_by_hash[h].append(rel)

    renamed = []  # list[(old_rel, new_rel)]
    for rel in list(added):
        h = new_hashes.get(rel)
        olds = deleted_by_hash.get(h, [])
        if h and len(olds) == 1:
            old_rel = olds.pop()
            renamed.append((old_rel, rel))
            added.remove(rel)
            deleted.remove(old_rel)

    # 5) Write updated manifest (stats + new text hashes when available)
    next_manifest = {"files": {}}
    for rel, st in curr_stats.items():
        if rel in new_hashes:
            next_manifest["files"][rel] = {
                "mtime": st["mtime"],
                "size": st["size"],
                "text_hash": new_hashes[rel],
            }
        else:
            if rel in prev:
                next_manifest["files"][rel] = prev[rel]
            else:
                next_manifest["files"][rel] = {
                    "mtime": st["mtime"],
                    "size": st["size"],
                }

    save_manifest(MANIFEST, next_manifest)

    # 6) Report
    print(f"Renamed/moved: {len(renamed)}")
    for o, n in renamed:
        print(f"  → {o}  ==>  {n}")
    print(f"Added: {len(added)}")
    for r in added:
        print("  +", r)
    print(f"Modified: {len(modified)}")
    for r in modified:
        print("  *", r)
    print(f"Deleted: {len(deleted)}")
    for r in deleted:
        print("  -", r)
    print(f"Unchanged: {unchanged}")
    print("\nManifest:", MANIFEST)

    return {
        "prev": prev,                          # old full state
        "state": next_manifest["files"],       # current files (stats + text_hash)
        "added": added,
        "modified": modified,
        "deleted": deleted,
        "renamed": renamed,
    }


# =========================
# PART 2: Embeddings / Chroma updates
# =========================

def update_embeddings(change_set):
    # --- tiny helper to keep Chroma happy ---
    def _clean_meta(md: dict) -> dict:
        """Drop keys whose values are None (Chroma disallows None in metadata)."""
        return {k: v for k, v in md.items() if v is not None}

    prev = change_set["prev"]              # previous manifest "files"
    state = change_set["state"]            # current manifest "files"
    added = change_set["added"]
    modified = change_set["modified"]
    deleted = change_set["deleted"]
    renamed = change_set["renamed"]

    # Setup embeddings + Chroma
    emb = OpenAIEmbeddings(model=EMBED_MODEL)
    vectordb = Chroma(
        collection_name=COLLECTION,
        embedding_function=emb,
        persist_directory=PERSIST_DIR,
    )

    # Index of chunk IDs that belong to files classified as DELETED (old manifest)
    deleted_ids_index = set()
    for rel in deleted:
        deleted_ids_index.update(prev.get(rel, {}).get("chunk_ids", []))

    moved_ids = set()  # track IDs we "move" so we don't delete later

    # A) ADDED → update-overlap (move) or add
    for rel in added:
        try:
            fhash = state.get(rel, {}).get("text_hash")
            if not fhash:
                print(f"[WARN] Skipping added (no text_hash): {rel}")
                continue
            mtime, size = state[rel]["mtime"], state[rel]["size"]
            ids, docs = build_docs_for_file(rel, fhash, mtime, size)
        except Exception as e:
            print(f"[WARN] Skipping added (read/build fail): {rel} ({e})")
            continue

        overlap_ids = [i for i in ids if i in deleted_ids_index]
        if overlap_ids:
            meta_by_id = {i: _clean_meta(d.metadata) for i, d in zip(ids, docs)}
            metadatas = [meta_by_id[i] for i in overlap_ids]
            # NOTE: internal API used to avoid re-embedding
            vectordb._collection.update(ids=overlap_ids, metadatas=metadatas)
            moved_ids.update(overlap_ids)

        add_ids = [i for i in ids if i not in overlap_ids]
        if add_ids:
            add_docs = [
                Document(page_content=doc.page_content, metadata=_clean_meta(doc.metadata))
                for i, doc in zip(ids, docs) if i in add_ids
            ]
            vectordb.add_documents(add_docs, ids=add_ids)

        # Record full current list for manifest (in-memory; will be saved below)
        state[rel]["chunk_ids"] = ids

    # B) MODIFIED → add new, delete removed, update metadata for unchanged (no re-embed)
    for rel in modified:
        try:
            fhash = state.get(rel, {}).get("text_hash")
            if not fhash:
                print(f"[WARN] Skipping modified (no text_hash): {rel}")
                continue
            mtime, size = state[rel]["mtime"], state[rel]["size"]
            ids, docs = build_docs_for_file(rel, fhash, mtime, size)
        except Exception as e:
            print(f"[WARN] Skipping modified (read/build fail): {rel} ({e})")
            continue

        new_ids = set(ids)
        old_ids = set(prev.get(rel, {}).get("chunk_ids", []))
        to_add = sorted(list(new_ids - old_ids))
        to_del = sorted(list(old_ids - new_ids))
        unchanged_ids = sorted(list(new_ids & old_ids))

        if to_add:
            add_docs = [
                Document(page_content=doc.page_content, metadata=_clean_meta(doc.metadata))
                for i, doc in zip(ids, docs) if i in to_add
            ]
            vectordb.add_documents(add_docs, ids=to_add)

        if to_del:
            vectordb.delete(ids=to_del)

        if unchanged_ids:
            meta_by_id = {i: _clean_meta(d.metadata) for i, d in zip(ids, docs)}
            metadatas = [meta_by_id[i] for i in unchanged_ids]
            vectordb._collection.update(ids=unchanged_ids, metadatas=metadatas)

        state[rel]["chunk_ids"] = ids

    # C) RENAMED / MOVED → metadata-only update or first-time ingest
    for old_rel, new_rel in renamed:
        chunk_ids = prev.get(old_rel, {}).get("chunk_ids", [])
        if not chunk_ids:
            # First-time ingest (content existed before but wasn't embedded)
            try:
                fhash = state.get(new_rel, {}).get("text_hash")
                if not fhash:
                    print(f"[WARN] Skipping renamed (no text_hash): {new_rel}")
                    continue
                mtime, size = state[new_rel]["mtime"], state[new_rel]["size"]
                ids, docs = build_docs_for_file(new_rel, fhash, mtime, size)
                if ids:
                    add_docs = [
                        Document(page_content=doc.page_content, metadata=_clean_meta(doc.metadata))
                        for doc in docs
                    ]
                    vectordb.add_documents(add_docs, ids=ids)
                state[new_rel]["chunk_ids"] = ids
            except Exception as e:
                print(f"[WARN] Skipping renamed (read/build fail): {new_rel} ({e})")
                continue
        else:
            # Metadata-only update (no re-splitting)
            fhash = state.get(new_rel, {}).get("text_hash")
            if not fhash:
                print(f"[WARN] Skipping rename metadata update (no text_hash): {new_rel}")
                continue
            mtime, size = state[new_rel]["mtime"], state[new_rel]["size"]
            new_full_path = os.path.abspath(os.path.join(BASE, new_rel))
            ftype = file_type(new_rel)
            metadatas = []
            for cid in chunk_ids:
                md = {
                    "source_path": new_full_path.replace("\\", "/"),
                    "filename": os.path.basename(new_rel),
                    "file_text_hash": fhash,
                    "chunk_text_hash": cid,
                    "mtime": mtime,
                    "size": size,
                    "ingest_version": f"rcs:{CHUNK_SIZE}:{CHUNK_OVERLAP}|{EMBED_MODEL}",
                    "type": ftype,
                }
                if ftype == "pptx":
                    md["page_number"] = None
                metadatas.append(_clean_meta(md))
            vectordb._collection.update(ids=chunk_ids, metadatas=metadatas)
            state[new_rel]["chunk_ids"] = chunk_ids

        if new_rel in state and old_rel in state:
            del state[old_rel]

    # D) DELETED → remove only IDs that were not "moved"
    for rel in deleted:
        old_ids = set(prev.get(rel, {}).get("chunk_ids", []))
        to_delete = sorted(list(old_ids - moved_ids))
        if to_delete:
            vectordb.delete(ids=to_delete)
        if rel in state:
            del state[rel]

    # Save manifest with chunk_ids
    manifest_out = {
        "version": 1,
        "scanned_at": datetime.utcnow().isoformat(timespec="seconds") + "Z",
        "files": state,
    }
    save_manifest(MANIFEST, manifest_out)

    print("\nEmbedding/metadata updates complete.")
    print("Chroma dir:", os.path.abspath(PERSIST_DIR))
    print("Manifest:", MANIFEST)

    return manifest_out, vectordb


# =========================
# MAIN
# =========================

if __name__ == "__main__":
    print("=== Detecting changes in docs under:", BASE, "===\n")
    changes = detect_changes()
    print("\n=== Updating embeddings in Chroma ===\n")
    manifest_out, vectordb = update_embeddings(changes)
    print("\nDone.")
